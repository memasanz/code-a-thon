{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2c591d",
   "metadata": {},
   "source": [
    "# Azure ML - Sample Batch Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa455160",
   "metadata": {},
   "source": [
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load data from a remote source, to make predictions against that data using a previously registered ML model, and finally save that data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7678ce",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1a7619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.34.0 to work with mm-aml-dev2\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f353922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My current directory is : /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon\n",
      "My directory name is : code-a-thon\n",
      "My user directory name is: memasanz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = os.getcwd()\n",
    "print(\"My current directory is : \" + directory_path)\n",
    "folder_name = os.path.basename(directory_path)\n",
    "print(\"My directory name is : \" + folder_name)\n",
    "\n",
    "parent = os.path.dirname(directory_path)\n",
    "parent_folder_name = os.path.basename(parent)\n",
    "print(\"My user directory name is: \" + parent_folder_name)\n",
    "\n",
    "user = parent_folder_name\n",
    "experiment_name = parent_folder_name + '-004-code-a-thon-diabetes-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5162c6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "#use default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if user + '-diabetes-tabular-dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['./data/diabetes.parquet'], # Upload the diabetes csv files in /data\n",
    "                        target_path= user + '-diabetes-data/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)  \n",
    "    parquet_paths = [(default_ds, user + '-diabetes-data/*.parquet')]\n",
    "    tab_data_set  = Dataset.Tabular.from_parquet_files(path=parquet_paths)\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name= user + '-diabetes-tabular-dataset',\n",
    "                                description='diabetes data',\n",
    "                                tags = {'format':'parquet'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a05e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_dataset = pd.read_parquet('./data/diabetes.parquet')\n",
    "raw_dataset = raw_dataset.drop(['PatientID', 'Diabetic'], axis = 1)\n",
    "raw_dataset = raw_dataset.head(25)\n",
    "raw_dataset.to_parquet('./data/diabetes-raw.parquet', index = False)\n",
    "\n",
    "\n",
    "if user + '-diabetes-tabular-dataset-raw' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['./data/diabetes-raw.parquet'], # Upload the diabetes csv files in /data\n",
    "                        target_path= user + '-diabetes-data-raw/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "    parquet_paths = [(default_ds, user + '-diabetes-data-raw/diabetes-raw.parquet')]\n",
    "    tab_data_set  = Dataset.Tabular.from_parquet_files(path=parquet_paths)\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name= user + '-diabetes-tabular-dataset-raw',\n",
    "                                description='diabetes data',\n",
    "                                tags = {'format':'parquet'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd3ff36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, user + '-004-batch-inferencing-pipeline')\n",
    "\n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668abbc",
   "metadata": {},
   "source": [
    "### Create Compute Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829e4b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to create: memasanz-cluster\n",
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = user + \"-cluster\"\n",
    "cluster_name = cluster_name[-16:]\n",
    "print('trying to create: ' + cluster_name)\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2, idle_seconds_before_scaledown=1800)\n",
    "        compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        compute_target.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e3608",
   "metadata": {},
   "source": [
    "## Run Configuration\n",
    "Create configuration for the running pipeline.  The RunConfiguration defines the environment used in the python steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93017f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
    "                                                                            pip_packages=['azureml-sdk','numpy', 'joblib', 'sklearn' ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866eb703",
   "metadata": {},
   "source": [
    "## Define Output Datasets\n",
    "\n",
    "Below are the configuration for datasets that will be passed between steps in our pipelien.  Note, in all cases we specifiy the datastore that should hold the datasets and wheather they should be registered following step completion or not.  This can optionally be disabled by removing the register_on_complete() call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad0e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencing_dataset = OutputFileDatasetConfig(name=user + '_004_diabetes_inferencing_dataset', destination=(default_ds, user + '_inferencing_dataset/{run-id}')).read_delimited_files().register_on_complete(name= user + '_004_pipeline_diabetes_inferencing_data')\n",
    "scored_dataset      = OutputFileDatasetConfig(name=user + '_004_diabetes_scored_dataset', destination=(default_ds, user + '_scored_dataset/{run-id}')).read_delimited_files().register_on_complete(name=user + '_004_pipeline_diabetes_scored_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bd02a",
   "metadata": {},
   "source": [
    "# Define Pipeline Parameters\n",
    "\n",
    "PipelineParameter objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Multiple pipeline parameters can be created and used. Included here are multiple sample pipeline parameters (get_data_param_*) to highlight how parameters can be passed into and consumed by various pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3661a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_parm  = PipelineParameter(name='model_name', default_value= user + '-diabetes_code-a-thon-model')\n",
    "user_param       = PipelineParameter(name='user_param', default_value= user)\n",
    "get_data_param_2 = PipelineParameter(name='get_data_param_2', default_value='value_2')\n",
    "get_data_param_3 = PipelineParameter(name='get_data_param_3', default_value='value_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af83f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon/batch-inferencing\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "folder_name = 'batch-inferencing'\n",
    "script_folder = os.path.join(os.getcwd(), folder_name)\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe846bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon/batch-inferencing/get_inferencing_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/get_inferencing_data.py\n",
    "\n",
    "  \n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Get Inferencing Data\")\n",
    "parser.add_argument('--user_param', type=str, required=True)\n",
    "parser.add_argument('--get_data_param_2', type=str, required=True)\n",
    "parser.add_argument('--get_data_param_3', type=str, required=True)\n",
    "parser.add_argument('--inferencing_dataset', dest='inferencing_dataset', required=True)\n",
    "\n",
    "# Note: the get_data_param args below are included only as an example of argument passing.\n",
    "# They are not consumed in the code sample shown here.\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "user_param = args.user_param\n",
    "get_data_param_2 = args.get_data_param_2\n",
    "get_data_param_3 = args.get_data_param_3\n",
    "inferencing_dataset = args.inferencing_dataset\n",
    "\n",
    "print(str(type(inferencing_dataset)))\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "dataset = Dataset.get_by_name(name =user_param + '-diabetes-tabular-dataset-raw', workspace = ws)\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "cols_to_keep = df.columns\n",
    "df = df[cols_to_keep]\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "print('saving inferencing data')\n",
    "# Save dataset for consumption in next pipeline step\n",
    "os.makedirs(inferencing_dataset, exist_ok=True)\n",
    "df.to_parquet(os.path.join(inferencing_dataset, 'inferencing_data.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6e4f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon/batch-inferencing/score_inferencing_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/score_inferencing_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import joblib\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "import pandas as pd\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "# Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Score Inferencing Data\")\n",
    "parser.add_argument('--model_name_parm', type=str, required=True)\n",
    "parser.add_argument('--scored_dataset', dest='scored_dataset', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "model_name = args.model_name_parm\n",
    "scored_dataset = args.scored_dataset\n",
    "\n",
    "# Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "# Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Get inferencing dataset\n",
    "inferencing_dataset = current_run.input_datasets['inferencing_data']\n",
    "inferencing_data_df = inferencing_dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "print('inferencing data df shape:' + str(inferencing_data_df.shape))\n",
    "\n",
    "print('model_name' + model_name)\n",
    "\n",
    "# Get model from workspace - the code below will always retrieve the latest version of the model; specific versions can be targeted.\n",
    "model_list = Model.list(ws, name=model_name, latest=True)\n",
    "model_path = model_list[0].download(exist_ok=True)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "print(inferencing_data_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions with new dataframe\n",
    "predictions = model.predict(inferencing_data_df)\n",
    "\n",
    "inferencing_data_df['Predictions']=predictions\n",
    "\n",
    "# Save scored dataset\n",
    "os.makedirs(scored_dataset, exist_ok=True)\n",
    "print(scored_dataset)\n",
    "\n",
    "inferencing_data_df.to_parquet(os.path.join(scored_dataset, 'scored_data.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d4eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon/batch-inferencing/publish_scored_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/publish_scored_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "# Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Get inferencing dataset\n",
    "scored_dataset = current_run.input_datasets['scored_data']\n",
    "scored_data_df = scored_dataset.to_pandas_dataframe()\n",
    "\n",
    "# Save dataset to ./outputs dir\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "scored_data_df.to_parquet(os.path.join('outputs', 'scored_data.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f419f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inferencing_data_step = PythonScriptStep(\n",
    "    name='Get Inferencing Data',\n",
    "    script_name='get_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--user_param', user_param,\n",
    "        '--get_data_param_2', get_data_param_2,\n",
    "        '--get_data_param_3', get_data_param_3,\n",
    "        '--inferencing_dataset', inferencing_dataset\n",
    "    ],\n",
    "    outputs=[inferencing_dataset],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "score_inferencing_data_step = PythonScriptStep(\n",
    "    name='Score Inferencing Data',\n",
    "    script_name='score_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--model_name_parm', model_name_parm,\n",
    "        '--scored_dataset', scored_dataset\n",
    "    ],\n",
    "    inputs=[inferencing_dataset.as_input(name='inferencing_data')],\n",
    "    outputs=[scored_dataset],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "publish_scored_data_step = PythonScriptStep(\n",
    "    name='Publish Scored Data',\n",
    "    script_name='publish_scored_data.py',\n",
    "    inputs=[scored_dataset.as_input(name='scored_data')],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=folder_name,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb44211",
   "metadata": {},
   "source": [
    "# Create Pipeline\n",
    "\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a915dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_inferencing_data_step, score_inferencing_data_step, publish_scored_data_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b31e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Get Inferencing Data [eeb4e83a][c3c3a643-cf84-4716-8049-ca3d41fc6b4f], (This step will run and generate new outputs)\n",
      "Created step Score Inferencing Data [dcf019b4][8e481714-0f22-4aa9-a1de-29fe73de6c5b], (This step will run and generate new outputs)\n",
      "Created step Publish Scored Data [63487ff5][00671c9f-05d5-4d6d-a671-ecb5752eb7b7], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun dda472f4-5c10-45b6-ab01-33db61b4ab45\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dda472f4-5c10-45b6-ab01-33db61b4ab45?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev2-rg/workspaces/mm-aml-dev2&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRunId: dda472f4-5c10-45b6-ab01-33db61b4ab45\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dda472f4-5c10-45b6-ab01-33db61b4ab45?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev2-rg/workspaces/mm-aml-dev2&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 3556711c-6571-4c80-86c2-46af8c708168\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/3556711c-6571-4c80-86c2-46af8c708168?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev2-rg/workspaces/mm-aml-dev2&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Get Inferencing Data ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_06fb54f016f9c5db7f6c722b877d5246194e215d9aab63bb43e1a0fc0b4db3c7_d.txt\n",
      "========================================================================================================================\n",
      "2021-11-08T15:53:05Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/mm-aml-dev2/azureml/3556711c-6571-4c80-86c2-46af8c708168/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/mm-aml-dev2/azureml/3556711c-6571-4c80-86c2-46af8c708168/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=24781 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/mm-aml-dev2/azureml/3556711c-6571-4c80-86c2-46af8c708168/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
      "2021-11-08T15:53:05Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/mm-aml-dev2/azureml/3556711c-6571-4c80-86c2-46af8c708168/mounts/workspaceblobstore\n",
      "2021-11-08T15:53:06Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-08T15:53:06Z Starting output-watcher...\n",
      "2021-11-08T15:53:06Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "2021-11-08T15:53:07Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
      "2021-11-08T15:53:07Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
      ">>>   \n",
      ">>>   \n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_8ceb6b8f58eaa8cee47c9719bba2cae4\n",
      "92473f7ef455: Pulling fs layer\n",
      "fb52bde70123: Pulling fs layer\n",
      "64788f86be3f: Pulling fs layer\n",
      "33f6d5f2e001: Pulling fs layer\n",
      "eeb715f1b6ae: Pulling fs layer\n",
      "fe519cf36537: Pulling fs layer\n",
      "58ff99196c15: Pulling fs layer\n",
      "9b13f06a8eff: Pulling fs layer\n",
      "2d4e93adbf58: Pulling fs layer\n",
      "6ee7c3767844: Pulling fs layer\n",
      "62cfc3ccb8ab: Pulling fs layer\n",
      "4a7af9d757ee: Pulling fs layer\n",
      "55ee3a8427d1: Pulling fs layer\n",
      "276b3b4ab7fb: Pulling fs layer\n",
      "409473831e40: Pulling fs layer\n",
      "1d385b5297a8: Pulling fs layer\n",
      "cdc39f9539df: Pulling fs layer\n",
      "f07fc542af21: Pulling fs layer\n",
      "08d3c92ca6ca: Pulling fs layer\n",
      "6ee7c3767844: Waiting\n",
      "62cfc3ccb8ab: Waiting\n",
      "4a7af9d757ee: Waiting\n",
      "55ee3a8427d1: Waiting\n",
      "276b3b4ab7fb: Waiting\n",
      "409473831e40: Waiting\n",
      "1d385b5297a8: Waiting\n",
      "cdc39f9539df: Waiting\n",
      "f07fc542af21: Waiting\n",
      "08d3c92ca6ca: Waiting\n",
      "33f6d5f2e001: Waiting\n",
      "eeb715f1b6ae: Waiting\n",
      "fe519cf36537: Waiting\n",
      "58ff99196c15: Waiting\n",
      "9b13f06a8eff: Waiting\n",
      "2d4e93adbf58: Waiting\n",
      "fb52bde70123: Verifying Checksum\n",
      "fb52bde70123: Download complete\n",
      "64788f86be3f: Verifying Checksum\n",
      "64788f86be3f: Download complete\n",
      "33f6d5f2e001: Verifying Checksum\n",
      "33f6d5f2e001: Download complete\n",
      "92473f7ef455: Verifying Checksum\n",
      "92473f7ef455: Download complete\n",
      "fe519cf36537: Verifying Checksum\n",
      "fe519cf36537: Download complete\n",
      "58ff99196c15: Verifying Checksum\n",
      "58ff99196c15: Download complete\n",
      "9b13f06a8eff: Verifying Checksum\n",
      "9b13f06a8eff: Download complete\n",
      "eeb715f1b6ae: Verifying Checksum\n",
      "eeb715f1b6ae: Download complete\n",
      "2d4e93adbf58: Verifying Checksum\n",
      "2d4e93adbf58: Download complete\n",
      "62cfc3ccb8ab: Verifying Checksum\n",
      "62cfc3ccb8ab: Download complete\n",
      "4a7af9d757ee: Verifying Checksum\n",
      "4a7af9d757ee: Download complete\n",
      "6ee7c3767844: Verifying Checksum\n",
      "6ee7c3767844: Download complete\n",
      "55ee3a8427d1: Verifying Checksum\n",
      "55ee3a8427d1: Download complete\n",
      "1d385b5297a8: Verifying Checksum\n",
      "1d385b5297a8: Download complete\n",
      "409473831e40: Verifying Checksum\n",
      "409473831e40: Download complete\n",
      "cdc39f9539df: Verifying Checksum\n",
      "cdc39f9539df: Download complete\n",
      "f07fc542af21: Verifying Checksum\n",
      "f07fc542af21: Download complete\n",
      "08d3c92ca6ca: Verifying Checksum\n",
      "08d3c92ca6ca: Download complete\n",
      "276b3b4ab7fb: Verifying Checksum\n",
      "276b3b4ab7fb: Download complete\n",
      "92473f7ef455: Pull complete\n",
      "fb52bde70123: Pull complete\n",
      "64788f86be3f: Pull complete\n",
      "33f6d5f2e001: Pull complete\n",
      "eeb715f1b6ae: Pull complete\n",
      "fe519cf36537: Pull complete\n",
      "58ff99196c15: Pull complete\n",
      "9b13f06a8eff: Pull complete\n",
      "2d4e93adbf58: Pull complete\n",
      "6ee7c3767844: Pull complete\n",
      "62cfc3ccb8ab: Pull complete\n",
      "4a7af9d757ee: Pull complete\n",
      "55ee3a8427d1: Pull complete\n",
      "276b3b4ab7fb: Pull complete\n",
      "409473831e40: Pull complete\n",
      "1d385b5297a8: Pull complete\n",
      "cdc39f9539df: Pull complete\n",
      "f07fc542af21: Pull complete\n",
      "08d3c92ca6ca: Pull complete\n",
      "Digest: sha256:2dd62cdffcf3989c82023b0938c78c6c0d0c0050ef236d24f3f67911568b7bd5\n",
      "Status: Downloaded newer image for viennaglobal.azurecr.io/azureml/azureml_8ceb6b8f58eaa8cee47c9719bba2cae4:latest\n",
      "viennaglobal.azurecr.io/azureml/azureml_8ceb6b8f58eaa8cee47c9719bba2cae4:latest\n",
      "2021-11-08T15:53:30Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
      "2021-11-08T15:53:30Z Check if container 3556711c-6571-4c80-86c2-46af8c708168_DataSidecar already exist exited with 0, \n",
      "\n",
      "5fd30f13f995beb4cc06ba0a362e0b943f74e053494aeef20fa79bfbc7dc9a67\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(ws, user + '-004-exp_batch_predictions')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263ad03",
   "metadata": {},
   "source": [
    "# Publish Pipeline\n",
    "\n",
    "Create a published version of pipeline that can be triggered via a REST API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = user + '004 Batch Prediction Pipeline',\n",
    "                                     description = 'Pipeline that generates batch predictions using a registered trained model.',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa11fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28048c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843eef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "print('Authentication header ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={\"ExperimentName\": user + \"rest-api-diabetes-batch\"})\n",
    "run_id = response.json()[\"Id\"]\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802704ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "published_pipeline_run = PipelineRun(ws.experiments[user + \"rest-api-diabetes-batch\"], run_id)\n",
    "\n",
    "# Block until the run completes\n",
    "published_pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fcda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
