{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fe633a",
   "metadata": {},
   "source": [
    "## Connect to your workspace\n",
    "\n",
    "To get started, connect to your workspace.\n",
    "\n",
    "> **Note**: If you haven't already established an authenticated session with your Azure subscription, you'll be prompted to authenticate by clicking a link, entering an authentication code, and signing into Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbb7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.34.0 to work with mm-aml-dev\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff1be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My current directory is : /mnt/batch/tasks/shared/LS_root/mounts/clusters/memasanz1/code/Users/memasanz/code-a-thon\n",
      "My directory name is : code-a-thon\n",
      "My user directory name is: memasanz\n",
      "user=memasanz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = os.getcwd()\n",
    "print(\"My current directory is : \" + directory_path)\n",
    "folder_name = os.path.basename(directory_path)\n",
    "print(\"My directory name is : \" + folder_name)\n",
    "\n",
    "parent = os.path.dirname(directory_path)\n",
    "parent_folder_name = os.path.basename(parent)\n",
    "print(\"My user directory name is: \" + parent_folder_name)\n",
    "\n",
    "user = parent_folder_name\n",
    "user = user.replace('_', '')\n",
    "user = user.replace('-', '')\n",
    "user = user[:10]\n",
    "print('user=' + user)\n",
    "experiment_name = parent_folder_name + '-003-code-a-thon-diabetes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7f137",
   "metadata": {},
   "source": [
    "## Prepare data for an experiment\n",
    "\n",
    "As you can see in the code above, it's easy to convert a tabular dataset to a Pandas dataframe, enabling you to work with the data using common python techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87eedc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already registered.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "#use default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "if user + '-diabetes-tabular-dataset' not in ws.datasets:\n",
    "    default_ds.upload_files(files=['./data/diabetes.parquet'], # Upload the diabetes csv files in /data\n",
    "                        target_path= user + '-diabetes-data/', # Put it in a folder path in the datastore\n",
    "                        overwrite=True, # Replace existing files of the same name\n",
    "                        show_progress=True)\n",
    "\n",
    "    #Create a tabular dataset from the path on the datastore (this may take a short while)  \n",
    "    parquet_paths = [(default_ds, user + '-diabetes-data/*.parquet')]\n",
    "    tab_data_set  = Dataset.Tabular.from_parquet_files(path=parquet_paths)\n",
    "\n",
    "    # Register the tabular dataset\n",
    "    try:\n",
    "        tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                name= user + '-diabetes-tabular-dataset',\n",
    "                                description='diabetes data',\n",
    "                                tags = {'format':'parquet'},\n",
    "                                create_new_version=True)\n",
    "        print('Dataset registered.')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print('Dataset already registered.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfadc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes-target\n",
      "pipeline_diabetes_scored_data\n",
      "pipeline_diabetes_inferencing_data\n",
      "diabetes-tabular-dataset-raw\n",
      "mm-synapse-base-NYC-20211103063643-dataset\n",
      "memasanz_004_pipeline_diabetes_scored_data\n",
      "memasanz_004_pipeline_diabetes_inferencing_data\n",
      "memasanz-diabetes-tabular-dataset-raw\n",
      "memasanz-diabetes-tabular-dataset\n",
      "mm-synapse-base-nyc_taxi4-20211103071528-dataset\n",
      "mm-synapse-base-NYC-20211103032007-dataset\n",
      "test3\n",
      "exp_Testing_Data\n",
      "exp_Training_Data\n",
      "exp_Raw_Data\n",
      "demodata\n",
      "demo\n",
      "test\n",
      "memasanztraining_registered_dataset\n",
      "memasanz_prepped_dataset\n",
      "registered_dataset\n",
      "herego\n",
      "inference-data-marathon-model-blob-3-marathon-aks-srv-insights-blob2\n",
      "scored_data\n",
      "inferencing_data\n",
      "compressor_outlier_clip_train_y\n",
      "compressor_outlier_clip_train_x\n",
      "compressor_outlier_clip_train\n",
      "compressor_outlier_clip\n",
      "compressors_holdout_dataset\n",
      "compressors_main_dataset\n",
      "marathon_compressor_raw\n",
      "inference-data-marathon-model-blob-2-marathon-aks-srv-insights-blob2\n",
      "inference-data-marathon-model-blob-2-marathon-aks-srv-insights-blob\n",
      "edspcleaned_credit_card_issueDataset\n",
      "edspcredit_card_issueDataset\n",
      "edspundermode\n",
      "edspSample_IssueDataset\n",
      "edspovermode\n",
      "edspmixedmode\n",
      "edspclustercentroids\n",
      "edspBasic\n",
      "ds4\n",
      "test_sample\n",
      "ds\n",
      "nyc dataset 12mo\n",
      "nyc datasetgreen2\n",
      "ds-nyc\n",
      "nyc datasetgreen\n",
      "nyc dataset\n",
      "nyc\n",
      "NYC Taxi & Limousine Commission - green taxi trip records\n",
      "label_ds\n",
      "input_images\n",
      "ratings\n",
      "memasanz-green-taxi-prepped\n",
      "diabetes target\n",
      "diabetes baseline\n",
      "TD-Demo-Clean_Missing_Data-Cleaning_transformation-385b4eaa\n",
      "MD-Demo-Train_Model-Trained_model-4a81bd4d\n",
      "megan_csv_table\n",
      "TD-Auto-Price-Training-Clean_Missing_Data-Cleaning_transformation-6badbd4d\n",
      "TD-Auto-Price-Training-Normalize_Data-Transformation_function-ea04431c\n",
      "MD-Auto-Price-Training-Train_Model-Trained_model-7df9d1ec\n",
      "bike-rentals\n",
      "diabetes file dataset\n",
      "diabetes dataset\n",
      "HoursCheck_20210212_150159\n",
      "dataset\n",
      "TaylorSalesByChannelAndProductForHCDivsions\n",
      "taylor_full\n",
      "taylor_v3\n",
      "taylor-10-cust_v2\n",
      "taylor-10-cust\n"
     ]
    }
   ],
   "source": [
    "# View Registered datasets in workspace\n",
    "all_datasets = Dataset.get_all(ws)\n",
    "\n",
    "for d in all_datasets:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95003b1d",
   "metadata": {},
   "source": [
    "### Create a file Dataset\n",
    "\n",
    "The dataset you created is a *tabular* dataset that can be read as a dataframe containing all of the data in the structured files that are included in the dataset definition. This works well for tabular data, but in some machine learning scenarios you might need to work with data that is unstructured; or you may simply want to handle reading the data from files in your own code. To accomplish this, you can use a *file* dataset, which creates a list of file paths in a virtual mount point, which you can use to read the data in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83542be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset from file path in default storage: /diabetes.parquet\n",
      "Datasets registered\n"
     ]
    }
   ],
   "source": [
    "#Create a file dataset from the path on the datastore (this may take a short while)\n",
    "file_data_set = Dataset.File.from_files(path=(default_ds, user + '-diabetes-data/*.parquet'))\n",
    "\n",
    "# Get the files in the dataset\n",
    "for file_path in file_data_set.to_path():\n",
    "    print('dataset from file path in default storage: ' + file_path)\n",
    "    \n",
    "# Register the file dataset\n",
    "try:\n",
    "    file_data_set = file_data_set.register(workspace=ws,\n",
    "                                            name= user + '-diabetes-file-dataset',\n",
    "                                            description='example diabetes files',\n",
    "                                            tags = {'format':'parquet'},\n",
    "                                            create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print('Datasets registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b2b44",
   "metadata": {},
   "source": [
    "## Create a training script\n",
    "\n",
    "Run the following two cells to create:\n",
    "\n",
    "1. A folder for a new experiment\n",
    "2. An training script file that uses **scikit-learn** to train a model and **matplotlib** to plot a ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839b90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memasanz-003-compute-cluster-diabetes_code-a-thon-exp folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = user + '-003-compute-cluster-diabetes_code-a-thon-exp'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b37bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing memasanz-003-compute-cluster-diabetes_code-a-thon-exp/diabetes_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "parser.add_argument(\"--input-data\", type=str, dest='training_dataset_id', help='training dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes data (passed as an input dataset)\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "run.log_image(name = \"ROC\", plot = fig)\n",
    "plt.show()\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10376f88",
   "metadata": {},
   "source": [
    "## Define an environment\n",
    "\n",
    "When you run a Python script as an experiment in Azure Machine Learning, a Conda environment is created to define the execution context for the script. Azure Machine Learning provides a default environment that includes many common packages; including the **azureml-defaults** package that contains the libraries necessary for working with an experiment run, as well as popular packages like **pandas** and **numpy**.\n",
    "\n",
    "You can also define your own environment in a Conda specification file, adding packages by using **conda** or **pip** to ensure your experiment has access to all the libraries it requires.\n",
    "\n",
    "> **Note**: The conda dependencies are installed first, followed by the pip dependencies. Since the **pip** package is required to install the pip dependencies, it's good practice to include it in the conda dependencies.\n",
    "\n",
    "Run the following cell to create a Conda specification file named *experiment_env.yml* in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f408aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing memasanz-003-compute-cluster-diabetes_code-a-thon-exp/experiment_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/experiment_env.yml\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb4910",
   "metadata": {},
   "source": [
    "Now you can use your custom conda environment file for your experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a85aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memasanz-experiment-env defined.\n",
      "name: experiment_env\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "- python=3.6.2\n",
      "- scikit-learn\n",
      "- ipykernel\n",
      "- matplotlib\n",
      "- pandas\n",
      "- pip\n",
      "- pip:\n",
      "  - azureml-defaults\n",
      "  - pyarrow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(user + \"-experiment-env\", experiment_folder + \"/experiment_env.yml\")\n",
    "\n",
    "# Let Azure ML manage dependencies\n",
    "experiment_env.python.user_managed_dependencies = False \n",
    "\n",
    "# Print the environment details\n",
    "print(experiment_env.name, 'defined.')\n",
    "print(experiment_env.python.conda_dependencies.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab0a18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c38a65ebe34e1a8db28c991e9bba87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Preparing\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-machine-learning-dev-rg/workspaces/mm-aml-dev&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\", \"run_id\": \"memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6\", \"run_properties\": {\"run_id\": \"memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6\", \"created_utc\": \"2021-11-29T18:04:46.582939Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"f720d388-ddcc-469d-bc38-1e16c2406b29\", \"azureml.git.repository_uri\": \"https://github.com/memasanz/code-a-thon\", \"mlflow.source.git.repoURL\": \"https://github.com/memasanz/code-a-thon\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"db5c51dcc4ac9df5abcaf03b28cefa97b8c30b4e\", \"mlflow.source.git.commit\": \"db5c51dcc4ac9df5abcaf03b28cefa97b8c30b4e\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Preparing\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://mmamldev2875344614.blob.core.windows.net/azureml/ExperimentRun/dcid.memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6/azureml-logs/60_control_log.txt?sv=2019-07-07&sr=b&sig=0CHoPWENbO9FWuwEtPGMe55qQJLNa%2FBS%2BsQG9OaxBdI%3D&skoid=83ede2c4-cce0-410e-b16f-c02f7a05745f&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2021-11-29T00%3A00%3A02Z&ske=2021-11-30T08%3A10%3A02Z&sks=b&skv=2019-07-07&st=2021-11-29T17%3A55%3A35Z&se=2021-11-30T02%3A05%3A35Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/60_control_log.txt\"]], \"run_duration\": \"0:00:54\", \"run_number\": \"2\", \"run_queued_details\": {\"status\": \"Preparing\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"[2021-11-29T18:04:47.034206] Using urllib.request Python 3.0 or later\\nStreaming log file azureml-logs/60_control_log.txt\\nRunning: ['/bin/bash', '/tmp/azureml_runs/memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6/azureml-environment-setup/conda_env_checker.sh']\\nStarting the daemon thread to refresh tokens in background for process with pid = 8736\\nMaterialized conda environment not found on target: /home/azureuser/.azureml/envs/azureml_0c5a9aa2def4b3c2501c1f40287a356b\\n\\n\\n[2021-11-29T18:04:47.141253] Logging experiment preparation status in history service.\\nRunning: ['/bin/bash', '/tmp/azureml_runs/memasanz-003-code-a-thon-diabetes-local_1638209085_949117a6/azureml-environment-setup/conda_env_builder.sh']\\nRunning: ['conda', '--version']\\nconda 4.10.3\\n\\nCreating conda environment...\\nRunning: ['conda', 'env', 'create', '-p', '/home/azureuser/.azureml/envs/azureml_0c5a9aa2def4b3c2501c1f40287a356b', '-f', 'azureml-environment-setup/mutated_conda_dependencies.yml']\\nCollecting package metadata (repodata.json): ...working... done\\nSolving environment: ...working... done\\n\\nDownloading and Extracting Packages\\nlibxcb-1.14          | 505 KB    | ########## | 100% \\nzstd-1.4.9           | 480 KB    | ########## | 100% \\nolefile-0.46         | 48 KB     | ########## | 100% \\nipython_genutils-0.2 | 27 KB     | ########## | 100% \\nipython-7.16.1       | 999 KB    | ########## | 100% \\nlibxml2-2.9.12       | 1.2 MB    | ########## | 100% \\nlcms2-2.12           | 312 KB    | ########## | 100% \\nlibffi-3.2.1         | 48 KB     | ########## | 100% \\nprompt-toolkit-3.0.2 | 259 KB    | ########## | 100% \\nexpat-2.4.1          | 168 KB    | ########## | 100% \\nncurses-6.0          | 781 KB    | ########## | 100% \\ntk-8.6.11            | 3.0 MB    | ########## | 100% \\ngstreamer-1.14.0     | 3.1 MB    | ########## | 100% \\ncertifi-2021.5.30    | 139 KB    | ########## | 100% \\nnumpy-base-1.19.2    | 4.1 MB    | ########## | 100% \\npyqt-5.9.2           | 4.5 MB    | ########## | 100% \\npandas-1.1.5         | 8.2 MB    | ########## | 100% \\nsip-4.19.8           | 274 KB    | ########## | 100% \\ndbus-1.13.18         | 504 KB    | ########## | 100% \\nglib-2.63.1          | 2.9 MB    | ########## | 100% \\njupyter_core-4.8.1   | 74 KB     | ########## | 100% \\npillow-8.3.1         | 637 KB    | ########## | 100% \\nzeromq-4.3.4         | 331 KB    | ########## | 100% \\nintel-openmp-2021.4. | 4.2 MB    | ########## | 100% \\nlibuuid-1.0.3        | 17 KB     | ########## | 100% \\nbackcall-0.2.0       | 13 KB     | ########## | 100% \\npyparsing-3.0.4      | 81 KB     | ########## | 100% \\nscipy-1.5.2          | 14.4 MB   | ########## | 100% \\nsqlite-3.23.1        | 808 KB    | ########## | 100% \\npyzmq-22.2.1         | 454 KB    | ########## | 100% \\nmkl_random-1.1.1     | 327 KB    | ########## | 100% \\nlibtiff-4.2.0        | 502 KB    | ########## | 100% \\nicu-58.2             | 10.5 MB   | ########## | 100% \\ndecorator-5.1.0      | 14 KB     | ########## | 100% \\nopenssl-1.0.2u       | 2.2 MB    | ########## | 100% \\nkiwisolver-1.3.1     | 80 KB     | ########## | 100% \\nblas-1.0             | 6 KB      | ########## | 100% \\nmkl_fft-1.3.0        | 170 KB    | ########## | 100% \\nparso-0.8.2          | 69 KB     | ########## | 100% \\nptyprocess-0.7.0     | 17 KB     | ########## | 100% \\nlibgfortran4-7.5.0   | 995 KB    | ########## | 100% \\nfontconfig-2.13.1    | 250 KB    | ########## | 100% \\nscikit-learn-0.24.2  | 5.2 MB    | ########## | 100% \\npygments-2.10.0      | 725 KB    | ########## | 100% \\npython-3.6.2         | 23.6 MB   | ########## | 100% \\ncycler-0.10.0        | 13 KB     | ########## | 100% \\nfreetype-2.11.0      | 618 KB    | ########## | 100% \\nlz4-c-1.9.3          | 185 KB    | ########## | 100% \\njoblib-1.0.1         | 208 KB    | ########## | 100% \\nlibwebp-base-1.2.0   | 437 KB    | ########## | 100% \\nthreadpoolctl-2.2.0  | 16 KB     | ########## | 100% \\njupyter_client-7.0.6 | 90 KB     | ########## | 100% \\njpeg-9d              | 232 KB    | ########## | 100% \\nreadline-7.0         | 848 KB    | ########## | 100% \\npython-dateutil-2.8. | 233 KB    | ########## | 100% \\nnumpy-1.19.2         | 22 KB     | ########## | 100% \\ngst-plugins-base-1.1 | 4.8 MB    | ########## | 100% \\nqt-5.9.6             | 67.3 MB   | ########## | 100% \\nlibgfortran-ng-7.5.0 | 22 KB     | ########## | 100% \\nipykernel-5.3.4      | 181 KB    | ########## | 100% \\ntraitlets-4.3.3      | 138 KB    | ########## | 100% \\nca-certificates-2021 | 115 KB    | ########## | 100% \\nmatplotlib-base-3.3. | 5.1 MB    | ########## | 100% \\npexpect-4.8.0        | 53 KB     | ########## | 100% \\nmkl-2020.2           | 138.3 MB  | ########## | 100% \\nmkl-service-2.3.0    | 52 KB     | ########## | 100% \\nwcwidth-0.2.5        | 26 KB     | ########## | 100% \\nnest-asyncio-1.5.1   | 10 KB     | ########## | 100% \\npcre-8.45            | 207 KB    | ########## | 100% \\nopenjpeg-2.4.0       | 331 KB    | ########## | 100% \\nlibsodium-1.0.18     | 244 KB    | ########## | 100% \\nentrypoints-0.3      | 12 KB     | ########## | 100% \\ntornado-6.1          | 581 KB    | ########## | 100% \\npytz-2021.3          | 171 KB    | ########## | 100% \\nlibpng-1.6.37        | 278 KB    | ########## | 100% \\nsix-1.16.0           | 18 KB     | ########## | 100% \\npickleshare-0.7.5    | 13 KB     | ########## | 100% \\nmatplotlib-3.3.4     | 26 KB     | ########## | 100% \\njedi-0.17.0          | 780 KB    | ########## | 100% \\nlibedit-3.1          | 151 KB    | ########## | 100% \\npip-21.2.2           | 1.8 MB    | ########## | 100% \\nsetuptools-58.0.4    | 788 KB    | ########## | 100% \\nPreparing transaction: ...working... done\\nVerifying transaction: ...working... done\\nExecuting transaction: ...working... \\n\\n    Installed package of scikit-learn can be accelerated using scikit-learn-intelex.\\n    More details are available here: https://intel.github.io/scikit-learn-intelex\\n\\n    For example:\\n\\n        $ conda install scikit-learn-intelex\\n        $ python -m sklearnex my_application.py\\n\\n    \\n\\ndone\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.34.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import azureml.core.runconfig\n",
    "from azureml.core import Environment, Experiment\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(user + '-diabetes-tabular-dataset')\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='diabetes_training.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n",
    "                                environment=experiment_env) \n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = user + '-003-code-a-thon-diabetes-local'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b98eb",
   "metadata": {},
   "source": [
    "## Create a compute cluster\n",
    "\n",
    "In many cases, your local compute resources may not be sufficient to process a complex or long-running experiment that needs to process a large volume of data; and you may want to take advantage of the ability to dynamically create and use compute resources in the cloud. Azure Machine Learning supports a range of compute targets, which you can define in your workpace and use to run experiments; paying for the resources only when using them.\n",
    "\n",
    "You can create a compute cluster in [Azure Machine Learning studio](https://ml.azure.com), or by using the Azure Machine Learning SDK. The following code cell checks your workspace for the existance of a compute cluster with a specified name, and if it doesn't exist, creates it.\n",
    "\n",
    "> **Important**: Change *your-compute-cluster* to a suitable name for your compute cluster in the code below before running it - you can specify the name of an existing cluster if you have one. Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61680775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = user + \"-cluster\"\n",
    "cluster_name = cluster_name[-16:]\n",
    "print('trying to create: ' + cluster_name)\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2, idle_seconds_before_scaledown=1800)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='diabetes_training.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n",
    "                                environment=experiment_env,\n",
    "                                compute_target=cluster_name) # Use docker to host environment\n",
    "\n",
    "\n",
    "\n",
    "#submit the experiment\n",
    "experiment_name = user + '-003-code-a-thon-diabetes-remote'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d26d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
